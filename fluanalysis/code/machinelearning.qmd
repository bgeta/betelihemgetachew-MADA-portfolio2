---
title: "Machine Learning"
editor: visual
---

Load packages

```{r}
library(dotwhisker)#for visualizing regression results 
library(skimr) #for visualization
library(here) #data loading/saving
library(dplyr)#data cleaning and processing
library(tidyr) #data cleaning and processing
library(tidymodels) #for modeling
library(gmodels)#for tables
library(ggplot2)#for hisograms and charts
library(performance)
library(tidyverse)
library(yardstick)
library(dotwhisker)# for visualizing regresssion results
library(broom.mixed)#for converting bayesian models to tidy tibbles
library(readr)
library(vip) #for variable importance plots
#library(rpart.plot)#for visualizing decision tree...note: WAS NOT ABLE TO INSTALL PACKAGE 
#library(glmnet)  WAS NOT ABLE TO INSTALL PACKAGE 
#library(ranger) WAS NOT ABLE TO INSTALL PACKAGE 
library(caret)
library(vip)
```

```{r}
install.packages("caret")
library(caret)
```

#Load most recent version of the dataset

```{r}
data_location <- here::here("fluanalysis","processed_data","Processed_data.RDS")
```

```{r}
NewClean_df<- readRDS(data_location)
```

#checking the variables to make sure there are no identical variables. These variables are strongly correlated and lead to poor model performance. so we need to remove any identical variables. Also source of error "Predictions from a rank deficient fit may be misleading".

```{r}
view(NewClean_df)
```

After reviweing the dataset, four variables are identified as identical and thus we need to remove them.

```{r}
NewClean_df <- subset(NewClean_df, select=-c(WeaknessYN,CoughYN,CoughYN2,MyalgiaYN))
```

#check if referenced variables are removed

```{r}
view(NewClean_df)
```

#remove unbalanced predictors that are not helpful in fitting/predicting the outcome. We are now planning to remove all binary predictors with less than 50 observations. lets look at the summary so we can manually identify and remove them.

```{r}
summary(NewClean_df)
```

#The summary shows that two variables, Hearing and Vision have less than 50 observations in one of the two response values. Thus they need to be removed.

```{r}
NewClean_df <- subset(NewClean_df, select=-c(Hearing,Vision))
```

#now lets check if the two variables are removed

```{r}
dim(NewClean_df)

```

#It looks like we have 730 obervatios and 26 variables.

# Analysis Code

# Data Setup

Data Setup #select of the data and save into training dataset, select 70 percent for training and 30% for testing. Also use the outcome BodyTemp as stratification for a more balanced outcome in the training and testing datasets

```{r}
set.seed(123)
data_split <- initial_split(NewClean_df, prop=c(.7),Strata=BodyTemp)
```

#here we will split data for training and testing

```{r}
train_data <- training(data_split)
test_data <- testing(data_split)
```

#take the train dataset and divide into 5 subsets using the vfold function

```{r}
folds<-vfold_cv(data=train_data, v=5, strata=BodyTemp)
```

#create a receipe for the data and fitting

```{r}
fluCV_rec <-
  recipe (BodyTemp ~., data=train_data  )%>%
step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

#Create null model #create receipe for null code didnt work 

#```{r}
#nullrecipe<- recipe(BodyTemp ~ NULL, data=train_data )%>%
# step_dummy(all_nominal_predictors()) %>%
#```

#create model for null

```{r}
null_model<- null_model()%>%
  set_engine("parsnip")%>%
  set_mode ("regression")

```

#Create worflow for null by combining recipe null which didont work when rendering and model


#nullworkflow <- 
# workflow()%>%
#add_model(null_mode)
#add_recipe(nullrecipe)


# fit the null model using the null workflow, none of my codes for the null model would render # even though there was no error while running the chunks


#null_model_performance <- fit_resamples(nullworkflow, resamples = folds)


#RMSE Lets get a better view of all 5 evaluation results

#```{r}
#null_model_performance %>%
#collect_metrics(summarize = FALSE)
#```

#Lets get a better view of one summarized result. Code not working when rendered

#```{r}
#null_model_performance %>%
#collect_metrics(summarize = TRUE)
#```

#The average RMSE is 1.22 for the null model performance

#computing RMSE for both training and test model using the null model

#Model Tuning and Fitting #Decision tree model

```{r}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
```

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```

#Model tuining with a GRID

```{r}
tree_wf <- workflow()%>%
  add_model(tune_spec)%>%
  add_recipe(fluCV_rec)
```

```{r}
tree_res <- tree_wf %>%
  tune_grid(resamples = folds, grid=tree_grid)
```

```{r}
tree_res %>%
collect_metrics(summarize = TRUE)
```

#RMSE is 1.22

```{r}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)

```

#show best tree model

```{r}
tree_res %>%
show_best("rmse")
```

```{r}
best_tree <- tree_res %>% 
select_best ("rmse")
```

```{r}
best_tree
```

#Finazling the model/Tuning process

```{r}
final_wf <- tree_wf %>%
finalize_workflow (best_tree)
```

```{r}
final_wf
```

#Finalziing the last fit to training data

```{r}
final_fit <- final_wf%>%
  fit(train_data)
```

**Fit LASSO MODEL** #I KEPT RUNING INTO ERRORS, DELETED A BUNCH OF CODES IN ORDER TO BE ABLE TO RENDER. I WASNT ABLE TO INSTALL RPART.PLOT, RANGER, GLMENT.

#Conduct LASSO to improve model prediction by potentially avoiding overfitting the model to the training data, and will also help us select the most important predictor variables
#create vector of potential mabda values (this are the tuning parameters)

#Build the model 

```{r}
Model1LASSO <- linear_reg (penalty = tune(), mixture=1) %>%
  set_engine("glmnet")
```
#Create the recipe ALREADY created in previus steps 

#create workflow

```{r}
wfLASSO <-
  workflow()%>%
  add_model(Model1LASSO) %>%
  add_recipe(fluCV_rec)

```

#Create THE GRID for Tuning 
```{r}
gridLASSO <- tibble (penalty= 10^seq (-4,-1,length.out=30))
```

```{r}
gridLASSO %>% top_n(-5) #basically picking the lowest penality values
```

```{r}
gridLASSO %>% top_n(5)   #highest penality values 
```
#Train and Tune the Model 

```{r}
LASSO_res <- 
  wfLASSO %>% 
  tune_grid(resamples=folds,
            grid = gridLASSO)
```

```{r}
LASSO_res %>% 
  collect_metrics()
```

```{r}
LASSO_res %>% show_best()
```
```{r}
bestmodel <- LASSO_res %>%
  select_best()
```
```{r}
bestmodel
```

#Final LASSO Model 
```{r}
Final_LASSO_WF <-
wfLASSO %>% finalize_workflow(bestmodel)
```

```{r}
Final_LASSO_fit <-
  Final_LASSO_WF %>%
  fit(train_data)
```
#I couldnt make the below code to work to actually get the rmse value, it kept saying collectmetrics doesnt exist for this type of object
#```{r}
#Final_LASSO_fit %>% 
#collect_metrics()
#```

```{r}
x <- extract_fit_engine(Final_LASSO_fit)
plot (x,"lambda")
```

**Random Forest***

#Build the model and improve training time 
```{r}
cores <- parallel::detectCores()
```

```{r}
cores
```
#Model for Random Forest
```{r}
modelRF <- 
    rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")
```
#Recipe for Random Forest, I used the same receipe pretty much for all the models except the null model. is that correct approach?

#workflow for Random Forest

```{r}
workflow_rf <-
  workflow() %>%
  add_model(modelRF)%>%
  add_recipe(fluCV_rec)
```
#Train and Tune Model 
```{r}
modelRF
```

```{r}
rf_res <- 
  workflow_rf %>% 
  tune_grid(folds)
```

```{r}
rf_res %>%
  show_best()
```
```{r}
best_rf <- rf_res %>%
  select_best ("rmse")

best_rf
```

```{r}
final_best_wf <- 
  workflow_rf %>%
  finalize_workflow(best_rf)
```
#Model Fit 
```{r}
final_fit_rf <-
  final_best_wf %>%
  fit(train_data)
```

```{r}
final_fit_rf
```
#Most of my codes stopped working for the various models, I deleted them in order to be able to render. Some of the packages I wasnt able to download etc., This exercise was actually the most challanging from me. I am not sure how to get the RMSE for the random forest or Lasso as the collect metrics for both didnt seem to work. I wasnt able to plot the importance matrix for the random forest. Please guide me to the repo of who over got this exercise right so i can clearly see my mistakes 


